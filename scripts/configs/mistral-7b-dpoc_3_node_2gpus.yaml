# Data args
data_dir: /home/hieum/uonlp/LLM_Emb/dataset/msmarco
train_file: msmarco_hard.jsonl
val_file: msmarco_dev.jsonl
num_workers: 20

# Model args
model_name_or_path: GritLM/GritLM-7B
use_bidirectional: False
attn_implementation: sdpa
normalized: True
pooling_method: mean
loss_gen_type: mixed
temperature: 0.07
# Lora setting
use_lora: True
train_adapter_name: msmarco
lora_r: 16
lora_alpha: 32
lora_dropout: 0.0
# Quantization setting
quantization: False

# Training args
# Machine args
nodes: 3
devices: 2
master_addr: localhost
master_port: "12345"

# Training args
seed: 1234
precision: bf16-true
mode: dpoc

# FSDP args
strategy: fsdp
sharding_strategy: shard_grad_op
use_cpu_offload: True
no_sync: True
low_memory: True

#Training data args
global_batch_size: 192
mini_batch_size: 1
max_seq_length: 2048
num_train_epochs: 2
# max_steps: 10000
num_positive_samples: 1
num_negative_samples: 64
topk_neg: 32

# Gradcache settings
use_gc: True 
gc_mini_batch_size: 2

# Objective args
prompt_loss_weight: 0.05
use_miner: False
dpo_loss_type: sigmoid
dpo_beta: 0.1
label_smoothing_factor: 0.1

# Optimizer args
learning_rate: 0.0003 # 3e-4 (5e-5@32)
warmup_steps: 50
apply_gradient_clipping: False
grad_norm_clip: 0.3
gradient_checkpointing: True

# Checkpointing args
logger_name: wandb
output_dir: output/checkpoints/mistral-7b-dpoc_msmarco/3n2g
save_interval: 50
log_interval: 1





