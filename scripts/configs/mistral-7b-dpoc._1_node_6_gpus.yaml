# Data args
data_dir: /home/hieum/uonlp/LLM_Emb/dataset/msmarco
train_file: msmarco_hard.jsonl
val_file: msmarco_dev.jsonl
num_workers: 20

# Model args
model_name_or_path: GritLM/GritLM-7B
use_bidirectional: False
attn_implementation: sdpa
normalized: True
pooling_method: mean
loss_gen_type: mixed
temperature: 0.07
use_lora: True
# Lora setting
train_adapter_name: msmarco
lora_r: 8
lora_alpha: 32
lora_dropout: 0.0
# Quantization setting
torch_dtype: bfloat16
quantization: 4

# Training args
mode: dpoc
output_dir: /home/hieum/uonlp/LLM_Emb/output/checkpoints/mistral-7b-dpoc_msmarco
devices: 6
nodes: 1
precision: bf16-true
strategy: ddp
# General quantization setting
quantize: None 
# Gradcache settings
use_gc: True 
gc_mini_batch_size: 1
mini_batch_size: 1
# Batch args
global_batch_size: 192 
max_seq_length: 2048
num_positive_samples: 1
num_negative_samples: 32
topk_neg: 4
# Optimizer args
use_miner: False
dpo_loss_type: sigmoid
dpo_beta: 0.1
prompt_loss_weight: 0.05
learning_rate: 0.0002 # 2e-4
num_train_epochs: 3
warmup_steps: 100
max_steps: 5000
# Checkpointing settings
logger_name: wandb
save_interval: 1000
log_interval: 1





