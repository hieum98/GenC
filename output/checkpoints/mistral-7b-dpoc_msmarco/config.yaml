adam_beta1: 0.9
adam_beta2: 0.999
devices: 2
dpo_beta: 0.1
dpo_loss_type: sigmoid
gc_mini_batch_size: 1
global_batch_size: 192
gradient_checkpointing: false
learning_rate: 0.0002
log_interval: 1
logger_name: wandb
max_seq_length: 2048
max_steps: 5000
mini_batch_size: 1
mode: dpoc
nodes: 1
num_negative_samples: 32
num_positive_samples: 1
num_train_epochs: 3
output_dir: /home/hieum/uonlp/LLM_Emb/output/checkpoints/mistral-7b-dpoc_msmarco
precision: bf16-true
prompt_loss_weight: 0.05
quantize: None
save_interval: 1000
seed: 2708
strategy: ddp
topk_neg: 4
use_gc: true
use_miner: false
warmup_steps: 100
weight_decay: 0.0
attn_implementation: sdpa
lora_alpha: 32
lora_dropout: 0.0
lora_r: 8
lora_weights_name_or_path: null
loss_gen_type: mixed
model_name_or_path: GritLM/GritLM-7B
normalized: true
pooling_method: mean
quantization: 4
temperature: 0.07
torch_dtype: bfloat16
train_adapter_name: msmarco
use_bidirectional: false
use_lora: true
data_dir: /home/hieum/uonlp/LLM_Emb/dataset/msmarco
ignore_index: -100
num_workers: 20
test_file: null
train_file: msmarco_hard.jsonl
val_file: msmarco_dev.jsonl
interval: 1000
max_iters: 1000
