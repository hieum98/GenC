adam_beta1: 0.9
adam_beta2: 0.999
apply_gradient_clipping: false
checkpoint_dir: null
devices: 4
dpo_beta: 0.1
dpo_loss_type: sigmoid
gc_mini_batch_size: 4
global_batch_size: 128
grad_norm_clip: 0.3
gradient_checkpointing: true
label_smoothing_factor: 0.1
learning_rate: 0.0002
log_interval: 1
logger_name: wandb
low_memory: true
master_addr: localhost
master_port: '12345'
max_seq_length: 2048
max_steps: 5000
mini_batch_size: 1
mode: dpoc
no_sync: true
nodes: 1
num_negative_samples: 32
num_positive_samples: 1
num_train_epochs: 3
output_dir: output/checkpoints/mistral-7b-dpoc_msmarco
precision: bf16-true
prompt_loss_weight: 0.05
reentrant_checkpointing: true
save_interval: 1000
seed: 1234
sharding_strategy: full_shard
strategy: fsdp
topk_neg: 16
use_activation_cpu_offload: false
use_cpu_offload: true
use_gc: true
use_miner: false
warmup_steps: 100
weight_decay: 0.0
attn_implementation: sdpa
lora_alpha: 32
lora_dropout: 0.0
lora_r: 16
lora_weights_name_or_path: null
loss_gen_type: mixed
model_name_or_path: GritLM/GritLM-7B
normalized: true
pooling_method: mean
quantization: true
temperature: 0.07
train_adapter_name: msmarco
use_bidirectional: false
use_lora: true
data_dir: /mnt/hieu/msmarco
ignore_index: -100
num_workers: 20
test_file: null
train_file: msmarco_hard.jsonl
val_file: msmarco_dev.jsonl
interval: 1000
max_iters: 1000
