{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import safetensors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig, \n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from transformers.utils import hub, SAFE_WEIGHTS_NAME, SAFE_WEIGHTS_INDEX_NAME\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from accelerate import init_empty_weights\n",
    "from bitsandbytes.nn import Linear4bit, Params4bit\n",
    "from peft import LoraConfig, PeftModel, TaskType, get_peft_model\n",
    "from fastcore.parallel import parallel\n",
    "\n",
    "from genc.model.genc import MistralEmbeddingLM\n",
    "from genc.trainer.loading_utils import load_and_quantize_parallel, n_loading_workers, replace_linear, setup_quantized_meta_for_peft, setup_quantized_peft_meta_for_training\n",
    "from genc.special_tokens import base_bos, user_bos, user_eos, embed_bos, embed_eos, assistant_bos, assistant_eos\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_model(\n",
    "        model_weights_name_or_path: str,\n",
    "        use_bidirectional: bool = False,\n",
    "        normalized: bool = True,\n",
    "        pooling_method: str = \"mean\",\n",
    "        loss_gen_type: str = \"mixed\",\n",
    "        temperature: float = 0.05,\n",
    "        quantization: bool = False,\n",
    "        use_lora: bool = False,\n",
    "        train_adapter_name: Optional[str] = \"default\",\n",
    "        lora_weights_name_or_path: Optional[str] = None,\n",
    "        lora_target_modules: Optional[List[str]] = None,\n",
    "        lora_r: Optional[int] = 8,\n",
    "        lora_alpha: Optional[int] = 16,\n",
    "        lora_dropout: Optional[float] = 0.05,\n",
    "        inference: bool = False,\n",
    "        low_memory: bool = True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        compute_dtype=torch.bfloat16,\n",
    "        precision: str = \"bf16\",\n",
    "        rank: int = 0,\n",
    "        local_rank: int = 0,\n",
    "        gradient_checkpointing: bool = False,\n",
    "        **kwargs,) -> Tuple[Union[PreTrainedModel, PeftModel], PreTrainedTokenizer]:\n",
    "    if lora_weights_name_or_path is not None and not use_lora:\n",
    "        logger.warning(\"You provided a path to LoRA weights but use_lora is set to False. We will set use_lora=True.\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_weights_name_or_path,\n",
    "        padding_side=\"right\", # Has to be right so masking of instruction tokens works correctly\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        if \"<|padding|>\" in tokenizer.get_vocab():\n",
    "            # StabilityLM specific fix\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"<|padding|>\"})\n",
    "        else:\n",
    "            logger.warning(\"Tokenizer does not have a pad token. We will use the bos token as pad token.\")\n",
    "            tokenizer.pad_token = tokenizer.bos_token\n",
    "            tokenizer.pad_token_id = tokenizer.bos_token_id\n",
    "    # Add special tokens into tokenizer\n",
    "    additional_special_tokens = [base_bos, user_bos, user_eos, embed_bos, embed_eos, assistant_bos, assistant_eos]\n",
    "    for item in additional_special_tokens:\n",
    "        if item in tokenizer.vocab:\n",
    "            additional_special_tokens.remove(item)\n",
    "    if len(additional_special_tokens) > 0:\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': additional_special_tokens})\n",
    "    new_vocab_size = len(tokenizer)\n",
    "\n",
    "    # Load model\n",
    "    logger.info(f\"Loading model from {model_weights_name_or_path}\")\n",
    "    # Load model config\n",
    "    if use_lora:\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_weights_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "            use_cache=False,\n",
    "            pretraining_tp=1,  # Fix mat1 and mat2 shapes cannot be multiplied  error with LLaMA-2\n",
    "            # See https://github.com/huggingface/transformers/pull/24906\n",
    "        )\n",
    "    else:\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_weights_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "    # Create the model\n",
    "    # Specify model args\n",
    "    model_args = [use_bidirectional, normalized, pooling_method, loss_gen_type, temperature, new_vocab_size]\n",
    "    if quantization is False:\n",
    "        model = MistralEmbeddingLM.from_pretrained(\n",
    "            model_weights_name_or_path,\n",
    "            *model_args,\n",
    "            config=config,\n",
    "            torch_dtype=torch_dtype,\n",
    "            **kwargs,\n",
    "        )\n",
    "        dtype = torch_dtype if precision == \"bf16\" else None\n",
    "        model.to(dtype=dtype, device=\"cpu\" if low_memory else local_rank)\n",
    "    elif inference:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16 if torch_dtype in [\"auto\", None] else torch_dtype,\n",
    "            )\n",
    "        model: PreTrainedModel = MistralEmbeddingLM.from_pretrained(\n",
    "                model_weights_name_or_path,\n",
    "                *model_args,\n",
    "                config=config,\n",
    "                # trust_remote_code=True,\n",
    "                quantization_config=bnb_config,\n",
    "                **kwargs,\n",
    "            )\n",
    "    else:\n",
    "        config.use_cache = False\n",
    "        if \"attn_implementation\" in kwargs:\n",
    "            config._attn_implementation = kwargs[\"attn_implementation\"]\n",
    "        # load model on meta device without calling init and replace nn.Linear with Linear4bit\n",
    "        with init_empty_weights():\n",
    "            model: MistralEmbeddingLM = MistralEmbeddingLM._from_config(\n",
    "                config,\n",
    "                use_bidirectional=use_bidirectional,\n",
    "                normalized=normalized,\n",
    "                pooling_method=pooling_method,\n",
    "                loss_gen_type=loss_gen_type,\n",
    "                temperature=temperature,\n",
    "                new_vocab_size=new_vocab_size,\n",
    "            )\n",
    "            model.model = replace_linear(model.model, Linear4bit, compute_dtype=compute_dtype,\n",
    "                                             quant_type='nf4', quant_storage=torch_dtype)\n",
    "        model.is_loaded_in_4bit = True\n",
    "        # Grab the safetensors files that hold the weights\n",
    "        try:\n",
    "            idx = hub.cached_file(model_weights_name_or_path, SAFE_WEIGHTS_INDEX_NAME)\n",
    "            files, _ = hub.get_checkpoint_shard_files(model_weights_name_or_path, idx)\n",
    "        except OSError:\n",
    "            try:\n",
    "                # This means the model doesn't have a model.safetensors.index.json because it is not sharded\n",
    "                files = []\n",
    "                files.append(hub.cached_file(model_weights_name_or_path, SAFE_WEIGHTS_NAME))\n",
    "            except OSError as e:\n",
    "                # This means the model probably doesn't have a safetensors file\n",
    "                raise e\n",
    "        quant_method = \"bnb\"\n",
    "        param_count = sum((p.numel() for n,p in model.named_parameters()))\n",
    "        n_workers = n_loading_workers(quant_method, param_count)\n",
    "        if rank == 0:\n",
    "            logger.info(f\"Using n_workers: {n_workers} for loading\")\n",
    "        start = time.time()\n",
    "        for filename in tqdm(files, desc=\"Loading & Quantizing Model Shards\", disable=rank!=0, position=0):\n",
    "            weights = safetensors.torch.load_file(filename)\n",
    "            parallel(load_and_quantize_parallel, iter(weights.items()), n_workers=n_workers, threadpool=True,\n",
    "                     model=model, dtype=torch_dtype, device=local_rank, skip_names=[],\n",
    "                     to_cpu=(low_memory and local_rank==0), to_meta=(low_memory and local_rank!=0),\n",
    "                     verbose=True, quant_method=quant_method)\n",
    "\n",
    "        if rank == 0:\n",
    "            logger.info(f\"Loaded model weights in {time.time()-start:.3f} seconds\")\n",
    "        # cleanup any extra memory usage from parallel loading\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"Rank {rank}: Model created: {torch.cuda.memory_reserved(local_rank)/2**30:.3f} GiB\")\n",
    "\n",
    "    # Load LoRA weights\n",
    "    if use_lora:\n",
    "        # PEFT will move quant_state to meta device, so this method prevents that\n",
    "        # from happening by replacing quant_state.to with a dummy function\n",
    "        if rank!=0 and low_memory:\n",
    "            setup_quantized_meta_for_peft(model)\n",
    "            \n",
    "        if lora_weights_name_or_path is None:\n",
    "            logger.info(\"No LoRA weights provided, we will use the default random LoRA weights.\")\n",
    "            if lora_target_modules == [\"all\"]:\n",
    "                logger.warning(\n",
    "                    \"You provided 'all' as target modules, we will use all the model to which LoRA can be applied.\"\n",
    "                )\n",
    "                lora_target_modules = [\"k_proj\", \"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"]\n",
    "            lora_config = LoraConfig(\n",
    "                r=lora_r,\n",
    "                lora_alpha=lora_alpha,\n",
    "                lora_dropout=lora_dropout,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                target_modules=lora_target_modules,\n",
    "                inference_mode=inference,\n",
    "            )\n",
    "            model: PeftModel = get_peft_model(model, lora_config, adapter_name=train_adapter_name)\n",
    "        else:\n",
    "            logger.info(f\"Loading pretrained LORA weights from {lora_weights_name_or_path}\")\n",
    "            model: PeftModel = PeftModel.from_pretrained(model, lora_weights_name_or_path, adapter_name=train_adapter_name, is_trainable=True)\n",
    "\n",
    "        if rank==0:\n",
    "            model.print_trainable_parameters()\n",
    "        elif low_memory:\n",
    "            # And then setup_quantized_peft_meta_for_training sets quant_state.to back to normal\n",
    "            setup_quantized_peft_meta_for_training(model)\n",
    "    \n",
    "    if len(additional_special_tokens) > 0:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        config.vocab_size += len(additional_special_tokens)\n",
    "        model.config.vocab_size = len(tokenizer)\n",
    "    \n",
    "    if gradient_checkpointing:\n",
    "        model.enable_input_require_grads()\n",
    "\n",
    "    if rank==0:\n",
    "        logger.info({\"memory/allocated_after_model_created\": torch.cuda.memory_allocated(local_rank)})\n",
    "        logger.info({\"memory/reserved_after_model_creation\": torch.cuda.memory_reserved(local_rank)})\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "    # General model setting\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "        )\n",
    "    ref_model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The path for reference model which is path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "        )\n",
    "    use_bidirectional: bool = field(default=False, metadata={\"help\": \"Whether to use bidirectional attention in compute encodings\"})\n",
    "    attn_implementation: str = field(default='sdpa', metadata={\"help\": \"eager/sdpa/flash_attention_2\"})\n",
    "    normalized: bool = field(default=True, metadata={\"help\": \"Whether to normalize the representations\"})\n",
    "    pooling_method: str = field(default='weightedmean', metadata={\"help\": \"Pooling method for passage. One of ['cls', 'lasttoken', 'mean', 'weightedmean']\"})\n",
    "    loss_gen_type: str = field(default=\"mixed\", metadata={\"help\": \"Type of gen loss: mixed/token\"})\n",
    "    temperature: float = field(\n",
    "        default=0.02, \n",
    "        metadata={\n",
    "            \"help\": \"Similarity will be sim = sim/temperature before using them to compute loss.\"\n",
    "            \" A higher temperature can reduce the value of similarity between texts in downstream tasks.\"\n",
    "            }\n",
    "            )\n",
    "    # Lora settings\n",
    "    use_lora: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to use LoRA. If True, the model will be trained with LoRA: https://arxiv.org/abs/2106.09685\"\n",
    "                )\n",
    "            },\n",
    "        )\n",
    "    train_adapter_name: Optional[str] = field(\n",
    "        default=\"default\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The name of the adapter to train. If not specified, the model will be trained from scratch.\"\n",
    "                )\n",
    "            },\n",
    "        )\n",
    "    lora_weights_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"If the model has been trained with LoRA, \"\n",
    "                \"path or huggingface hub name or local path to the pretrained weights.\"\n",
    "                )\n",
    "            },\n",
    "        )\n",
    "    lora_r: Optional[int] = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"Lora attention dimension.\"},\n",
    "        )\n",
    "    lora_alpha: Optional[float] = field(\n",
    "        default=64,\n",
    "        metadata={\"help\": \"The alpha parameter for Lora scaling.\"},\n",
    "        )\n",
    "    lora_dropout: Optional[float] = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"The dropout probability for Lora layers.\"},\n",
    "        )\n",
    "    \n",
    "    quantization: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={ \"help\": (\n",
    "                \"Whether to use '4' or '8' bit quantization. Requires bitsandbytes library:\"\n",
    "                \" https://github.com/TimDettmers/bitsandbytes. This parameter is only used for training.\"\n",
    "                )},\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.42s/it]\n",
      "You provided 'all' as target modules, we will use all the model to which LoRA can be applied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0: Model created: 0.000 GiB\n",
      "trainable params: 37,748,736 || all params: 7,279,480,832 || trainable%: 0.518563574397499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('checkpoint/7b-esft_msmarco-50/tokenizer_config.json',\n",
       " 'checkpoint/7b-esft_msmarco-50/special_tokens_map.json',\n",
       " 'checkpoint/7b-esft_msmarco-50/tokenizer.model',\n",
       " 'checkpoint/7b-esft_msmarco-50/added_tokens.json',\n",
       " 'checkpoint/7b-esft_msmarco-50/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import HfArgumentParser\n",
    "\n",
    "from genc.args import DataArguments, TrainingArguments, ValidationArgument\n",
    "\n",
    "config_file = \"output/esft_msmarco_7b/config.yaml\"\n",
    "checkpoint_path = \"output/esft_msmarco_7b/checkpoints/step_50.ckpt\"\n",
    "\n",
    "parser = HfArgumentParser((DataArguments, ModelArguments, TrainingArguments, ValidationArgument))\n",
    "data_args, model_args, training_args, validation_args = parser.parse_yaml_file(yaml_file=config_file)\n",
    "\n",
    "model, tokenizer = load_model(\n",
    "        model_weights_name_or_path=model_args.model_name_or_path,\n",
    "        use_bidirectional=model_args.use_bidirectional,\n",
    "        normalized=model_args.normalized,\n",
    "        pooling_method=model_args.pooling_method,\n",
    "        loss_gen_type=model_args.loss_gen_type,\n",
    "        temperature=model_args.temperature,\n",
    "        quantization=model_args.quantization,\n",
    "        use_lora=model_args.use_lora,\n",
    "        train_adapter_name=model_args.train_adapter_name,\n",
    "        lora_weights_name_or_path=model_args.lora_weights_name_or_path,\n",
    "        lora_target_modules=[\"all\"],\n",
    "        lora_r=model_args.lora_r,\n",
    "        lora_alpha=model_args.lora_alpha,\n",
    "        lora_dropout=model_args.lora_dropout,\n",
    "        inference=False,\n",
    "        low_memory=training_args.low_memory,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        compute_dtype=torch.bfloat16,\n",
    "        precision=training_args.precision,\n",
    "        rank=0,\n",
    "        local_rank=0,\n",
    "        gradient_checkpointing=training_args.gradient_checkpointing,\n",
    "        attn_implementation=model_args.attn_implementation,\n",
    "    )\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained('checkpoint/7b-esft_msmarco-50')\n",
    "tokenizer.save_pretrained('checkpoint/7b-esft_msmarco-50')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
