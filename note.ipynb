{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import safetensors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig, \n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from transformers.utils import hub, SAFE_WEIGHTS_NAME, SAFE_WEIGHTS_INDEX_NAME\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from accelerate import init_empty_weights\n",
    "from bitsandbytes.nn import Linear4bit, Params4bit\n",
    "from peft import LoraConfig, PeftModel, TaskType, get_peft_model\n",
    "from fastcore.parallel import parallel\n",
    "\n",
    "from genc.model.genc import MistralEmbeddingLM, LlamaEmbeddingLM, PhiEmbeddingLM\n",
    "from genc.model.lora_genc import LoRaGenc\n",
    "from genc.trainer.loading_utils import load_and_quantize_parallel, n_loading_workers, replace_linear, setup_quantized_meta_for_peft, setup_quantized_peft_meta_for_training\n",
    "from genc.trainer.trainer_utils import find_all_linear_names\n",
    "\n",
    "base_bos = \"<s>\",\n",
    "user_bos = \"<|user|>\\n\",\n",
    "user_eos = \"\",\n",
    "embed_bos = \"\\n<|embed|>\\n\",\n",
    "embed_eos = \"</e>\",\n",
    "assistant_bos = \"\\n<|assistant|>\\n\",\n",
    "assistant_eos = \"</s>\",\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_model(\n",
    "        model_weights_name_or_path: str,\n",
    "        pretrained_type: str,\n",
    "        use_bidirectional: bool = False,\n",
    "        normalized: bool = True,\n",
    "        pooling_method: str = \"mean\",\n",
    "        loss_gen_type: str = \"mixed\",\n",
    "        temperature: float = 0.05,\n",
    "        quantization: bool = False,\n",
    "        use_lora: bool = False,\n",
    "        emb_adapter_name: Optional[str] = \"emb\",\n",
    "        gen_adapter_name: Optional[str] = \"gen\",\n",
    "        lora_target_modules: Optional[List[str]] = None,\n",
    "        lora_r: Optional[int] = 8,\n",
    "        lora_alpha: Optional[int] = 16,\n",
    "        lora_dropout: Optional[float] = 0.05,\n",
    "        inference: bool = False,\n",
    "        low_memory: bool = True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        compute_dtype=torch.bfloat16,\n",
    "        precision: str = \"bf16\",\n",
    "        rank: int = 0,\n",
    "        local_rank: int = 0,\n",
    "        gradient_checkpointing: bool = False,\n",
    "        **kwargs,) -> Tuple[Union[PreTrainedModel, PeftModel], PreTrainedTokenizer]:\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_weights_name_or_path,\n",
    "        padding_side=\"right\", # Has to be right so masking of instruction tokens works correctly\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        if \"<|padding|>\" in tokenizer.get_vocab():\n",
    "            # StabilityLM specific fix\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"<|padding|>\"})\n",
    "        else:\n",
    "            logger.warning(\"Tokenizer does not have a pad token. We will use the bos token as pad token.\")\n",
    "            tokenizer.pad_token = tokenizer.bos_token\n",
    "            tokenizer.pad_token_id = tokenizer.bos_token_id\n",
    "    # Add special tokens into tokenizer\n",
    "    additional_special_tokens = [base_bos, user_bos, user_eos, embed_bos, embed_eos, assistant_bos, assistant_eos]\n",
    "    for item in additional_special_tokens:\n",
    "        if item in tokenizer.vocab:\n",
    "            additional_special_tokens.remove(item)\n",
    "    if len(additional_special_tokens) > 0:\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': additional_special_tokens})\n",
    "    new_vocab_size = len(tokenizer)\n",
    "\n",
    "    # Load model\n",
    "    logger.info(f\"Loading model from {model_weights_name_or_path}\")\n",
    "    # Load model config\n",
    "    if use_lora:\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_weights_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "            use_cache=False,\n",
    "            pretraining_tp=1,  # Fix mat1 and mat2 shapes cannot be multiplied  error with LLaMA-2\n",
    "            # See https://github.com/huggingface/transformers/pull/24906\n",
    "        )\n",
    "    else:\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_weights_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "    # Create the model\n",
    "    # Specify model args\n",
    "    model_args = [use_bidirectional, normalized, pooling_method, loss_gen_type, temperature, tokenizer]\n",
    "    if pretrained_type == 'llama':\n",
    "        model_class = LlamaEmbeddingLM\n",
    "    elif pretrained_type == 'mistral':\n",
    "        model_class = MistralEmbeddingLM\n",
    "    elif pretrained_type == 'phi':\n",
    "        model_class = PhiEmbeddingLM\n",
    "    else:\n",
    "        raise ValueError(f\"Model type not recognized: {pretrained_type}\")\n",
    "    \n",
    "    if quantization is False:\n",
    "        model = model_class.from_pretrained(\n",
    "            model_weights_name_or_path,\n",
    "            *model_args,\n",
    "            config=config,\n",
    "            torch_dtype=torch_dtype,\n",
    "            **kwargs,\n",
    "        )\n",
    "        dtype = torch_dtype if precision == \"bf16\" else None\n",
    "        model.to(dtype=dtype, device=\"cpu\" if low_memory else local_rank)\n",
    "    elif inference:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16 if torch_dtype in [\"auto\", None] else torch_dtype,\n",
    "            )\n",
    "        model: PreTrainedModel = MistralEmbeddingLM.from_pretrained(\n",
    "                model_weights_name_or_path,\n",
    "                *model_args,\n",
    "                config=config,\n",
    "                # trust_remote_code=True,\n",
    "                quantization_config=bnb_config,\n",
    "                **kwargs,\n",
    "            )\n",
    "    else:\n",
    "        config.use_cache = False\n",
    "        if \"attn_implementation\" in kwargs:\n",
    "            config._attn_implementation = kwargs[\"attn_implementation\"]\n",
    "        # load model on meta device without calling init and replace nn.Linear with Linear4bit\n",
    "        with init_empty_weights():\n",
    "            model: MistralEmbeddingLM = MistralEmbeddingLM._from_config(\n",
    "                config,\n",
    "                use_bidirectional=use_bidirectional,\n",
    "                normalized=normalized,\n",
    "                pooling_method=pooling_method,\n",
    "                loss_gen_type=loss_gen_type,\n",
    "                temperature=temperature,\n",
    "                new_vocab_size=new_vocab_size,\n",
    "            )\n",
    "            model.model = replace_linear(model.model, Linear4bit, compute_dtype=compute_dtype,\n",
    "                                             quant_type='nf4', quant_storage=torch_dtype)\n",
    "        model.is_loaded_in_4bit = True\n",
    "        # Grab the safetensors files that hold the weights\n",
    "        try:\n",
    "            idx = hub.cached_file(model_weights_name_or_path, SAFE_WEIGHTS_INDEX_NAME)\n",
    "            files, _ = hub.get_checkpoint_shard_files(model_weights_name_or_path, idx)\n",
    "        except OSError:\n",
    "            try:\n",
    "                # This means the model doesn't have a model.safetensors.index.json because it is not sharded\n",
    "                files = []\n",
    "                files.append(hub.cached_file(model_weights_name_or_path, SAFE_WEIGHTS_NAME))\n",
    "            except OSError as e:\n",
    "                # This means the model probably doesn't have a safetensors file\n",
    "                raise e\n",
    "        quant_method = \"bnb\"\n",
    "        param_count = sum((p.numel() for n,p in model.named_parameters()))\n",
    "        n_workers = n_loading_workers(quant_method, param_count)\n",
    "        if rank == 0:\n",
    "            logger.info(f\"Using n_workers: {n_workers} for loading\")\n",
    "        start = time.time()\n",
    "        for filename in tqdm(files, desc=\"Loading & Quantizing Model Shards\", disable=rank!=0, position=0):\n",
    "            weights = safetensors.torch.load_file(filename)\n",
    "            parallel(load_and_quantize_parallel, iter(weights.items()), n_workers=n_workers, threadpool=True,\n",
    "                     model=model, dtype=torch_dtype, device=local_rank, skip_names=[],\n",
    "                     to_cpu=(low_memory and local_rank==0), to_meta=(low_memory and local_rank!=0),\n",
    "                     verbose=True, quant_method=quant_method)\n",
    "\n",
    "        if rank == 0:\n",
    "            logger.info(f\"Loaded model weights in {time.time()-start:.3f} seconds\")\n",
    "        # cleanup any extra memory usage from parallel loading\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"Rank {rank}: Model created: {torch.cuda.memory_reserved(local_rank)/2**30:.3f} GiB\")\n",
    "\n",
    "    # Load LoRA weights\n",
    "    if use_lora:\n",
    "        # PEFT will move quant_state to meta device, so this method prevents that\n",
    "        # from happening by replacing quant_state.to with a dummy function\n",
    "        assert emb_adapter_name is not None or gen_adapter_name is not None, \"You must provide at least one adapter name\"\n",
    "\n",
    "        if lora_target_modules == [\"all\"]:\n",
    "            print(\"You provided 'all' as target modules, we will use all the model to which LoRA can be applied.\")\n",
    "            lora_target_modules = find_all_linear_names(model, quantization=quantization)\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            target_modules=lora_target_modules,\n",
    "            modules_to_save = [],\n",
    "            inference_mode=inference,\n",
    "        )\n",
    "        if emb_adapter_name is not None:\n",
    "            model = LoRaGenc(\n",
    "                model=model,\n",
    "                lora_config=lora_config,\n",
    "                adapter_name=emb_adapter_name\n",
    "            )\n",
    "        if gen_adapter_name is not None and gen_adapter_name != emb_adapter_name:\n",
    "            if isinstance(model, LoRaGenc):\n",
    "                model.add_adapter(adapter_name=gen_adapter_name, peft_config=lora_config)\n",
    "            else:\n",
    "                model = LoRaGenc(\n",
    "                    model=model,\n",
    "                    lora_config=lora_config,\n",
    "                    adapter_name=gen_adapter_name\n",
    "                )\n",
    "        # Always set the emb adapter as the current adapter\n",
    "        model.set_adapter(emb_adapter_name)\n",
    "        \n",
    "        if rank==0:\n",
    "            model.print_trainable_parameters()\n",
    "    \n",
    "    if len(additional_special_tokens) > 0:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        config.vocab_size += len(additional_special_tokens)\n",
    "        model.config.vocab_size = len(tokenizer)\n",
    "    \n",
    "    if gradient_checkpointing:\n",
    "        model.enable_input_require_grads()\n",
    "\n",
    "    if rank==0:\n",
    "        logger.info({\"memory/allocated_after_model_created\": torch.cuda.memory_allocated(local_rank)})\n",
    "        logger.info({\"memory/reserved_after_model_creation\": torch.cuda.memory_reserved(local_rank)})\n",
    "    \n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.42s/it]\n",
      "You provided 'all' as target modules, we will use all the model to which LoRA can be applied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0: Model created: 0.000 GiB\n",
      "trainable params: 37,748,736 || all params: 7,279,480,832 || trainable%: 0.518563574397499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('checkpoint/7b-esft_msmarco-50/tokenizer_config.json',\n",
       " 'checkpoint/7b-esft_msmarco-50/special_tokens_map.json',\n",
       " 'checkpoint/7b-esft_msmarco-50/tokenizer.model',\n",
       " 'checkpoint/7b-esft_msmarco-50/added_tokens.json',\n",
       " 'checkpoint/7b-esft_msmarco-50/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import HfArgumentParser\n",
    "\n",
    "from genc.args import DataArguments, TrainingArguments, ValidationArgument, ModelArguments\n",
    "\n",
    "config_file = \"esft_8b_instruct/config.yaml\"\n",
    "checkpoint_path = \"esft_8b_instruct/checkpoints/step_1400.ckpt\"\n",
    "\n",
    "parser = HfArgumentParser((DataArguments, ModelArguments, TrainingArguments, ValidationArgument))\n",
    "data_args, model_args, training_args, validation_args = parser.parse_yaml_file(yaml_file=config_file)\n",
    "\n",
    "model, tokenizer = load_model(\n",
    "        model_weights_name_or_path=model_args.model_name_or_path,\n",
    "        use_bidirectional=model_args.use_bidirectional,\n",
    "        normalized=model_args.normalized,\n",
    "        pooling_method=model_args.pooling_method,\n",
    "        loss_gen_type=model_args.loss_gen_type,\n",
    "        temperature=model_args.temperature,\n",
    "        quantization=model_args.quantization,\n",
    "        use_lora=model_args.use_lora,\n",
    "        train_adapter_name=model_args.train_adapter_name,\n",
    "        lora_weights_name_or_path=model_args.lora_weights_name_or_path,\n",
    "        lora_target_modules=[\"all\"],\n",
    "        lora_r=model_args.lora_r,\n",
    "        lora_alpha=model_args.lora_alpha,\n",
    "        lora_dropout=model_args.lora_dropout,\n",
    "        inference=False,\n",
    "        low_memory=training_args.low_memory,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        compute_dtype=torch.bfloat16,\n",
    "        precision=training_args.precision,\n",
    "        rank=0,\n",
    "        local_rank=0,\n",
    "        gradient_checkpointing=training_args.gradient_checkpointing,\n",
    "        attn_implementation=model_args.attn_implementation,\n",
    "    )\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained('checkpoint/edpo_8b_instruct')\n",
    "tokenizer.save_pretrained('checkpoint/edpo_8b_instruct')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
